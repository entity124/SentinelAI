{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9ffd7d04-e4f3-4eb3-9a53-58abd8d79eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries installed successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install \"pandas<2.2.0\" \"scikit-learn==1.3.2\" \"ibm-watsonx-ai\" -U -q\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"Libraries installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0d11f52a-f5ca-4c05-9aaa-74adcc5852c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project connected for file transfer.\n"
     ]
    }
   ],
   "source": [
    "# --- AUTHENTICATION ---\n",
    "credentials = {\n",
    "    \"url\": \"https://ca-tor.ml.cloud.ibm.com\", \n",
    "    \"apikey\": \"OP1mOPIL34elWjPUuCyDwsPl-3lUMt8MOYm3NKjgAyeO\"\n",
    "}\n",
    "project_id = \"a0cfb053-a3b6-4b55-9d97-8348e29c8a98\"\n",
    "\n",
    "# Initialize Client \n",
    "client = APIClient(credentials)\n",
    "client.set.default_project(project_id)\n",
    "\n",
    "print(\"Project connected for file transfer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5c5fa280-6e99-45eb-93e0-0db8eabca206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for mock_transactions.csv...\n",
      "Successfully saved data asset content to file: 'mock_transactions.csv'\n",
      "SUCCESS: mock_transactions.csv downloaded.\n",
      "Searching for live_model_input.csv...\n",
      "Successfully saved data asset content to file: 'live_model_input.csv'\n",
      "SUCCESS: live_model_input.csv downloaded.\n",
      "Searching for mock_transactions-2.csv...\n",
      "Successfully saved data asset content to file: 'mock_transactions-2.csv'\n",
      "SUCCESS: mock_transactions-2.csv downloaded.\n",
      "Searching for live_model_input_for_2.csv...\n",
      "Successfully saved data asset content to file: 'live_model_input_for_2.csv'\n",
      "SUCCESS: live_model_input_for_2.csv downloaded.\n",
      "Searching for Kinghacks governance sheet.csv...\n",
      "Successfully saved data asset content to file: 'Kinghacks governance sheet.csv'\n",
      "SUCCESS: Kinghacks governance sheet.csv downloaded.\n",
      "Merging datasets...\n",
      "Data Ready. Total Rows: 24794\n"
     ]
    }
   ],
   "source": [
    "# Download Files\n",
    "def download_from_project(filename):\n",
    "    print(f\"Searching for {filename}...\")\n",
    "    assets = client.data_assets.get_details()\n",
    "    asset_id = next((item['metadata']['asset_id'] for item in assets['resources'] \n",
    "                     if item['metadata']['name'] == filename), None)\n",
    "    if asset_id:\n",
    "        client.data_assets.download(asset_id, filename)\n",
    "        print(f\"SUCCESS: {filename} downloaded.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Could not find {filename}.\")\n",
    "\n",
    "download_from_project(\"mock_transactions.csv\")\n",
    "download_from_project(\"live_model_input.csv\")\n",
    "download_from_project(\"mock_transactions-2.csv\")\n",
    "download_from_project(\"live_model_input_for_2.csv\")\n",
    "download_from_project(\"Kinghacks governance sheet.csv\")\n",
    "\n",
    "# Merge Data\n",
    "print(\"Merging datasets...\")\n",
    "df_features = pd.read_csv(\"live_model_input.csv\")\n",
    "df_raw = pd.read_csv(\"mock_transactions.csv\")\n",
    "\n",
    "# Merge\n",
    "df_data = pd.merge(\n",
    "    df_features,\n",
    "    df_raw[['transaction_id', 'timestamp', 'user_id', 'merchant', 'pattern_label']], \n",
    "    left_on='Transaction_ID',\n",
    "    right_on='transaction_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Sort chronologically\n",
    "df_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\n",
    "df_data = df_data.sort_values(by=['user_id', 'merchant', 'timestamp'])\n",
    "\n",
    "print(f\"Data Ready. Total Rows: {len(df_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c64f0c96-9397-4576-b0d8-8dfec8147694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Local AI Model...\n",
      "SUCCESS: Model trained on local CPU.\n",
      "The AI has learned to identify predatory behavior based on your specific data.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Local AI Model...\")\n",
    "\n",
    "# Define Target: True if 'potential_trap', False if 'normal'\n",
    "df_data['target'] = df_data['pattern_label'] != 'normal'\n",
    "\n",
    "# Select Features (What the AI sees)\n",
    "features = ['Amount', 'Price_Change_Pct', 'Frequency', 'Category']\n",
    "X = df_data[features]\n",
    "y = df_data['target']\n",
    "\n",
    "# Build Pipeline (Preprocessing + Random Forest)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['Frequency', 'Category']),\n",
    "        ('num', 'passthrough', ['Amount', 'Price_Change_Pct'])\n",
    "    ])\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train\n",
    "model.fit(X, y)\n",
    "print(\"SUCCESS: Model trained on local CPU.\")\n",
    "print(\"The AI has learned to identify predatory behavior based on your specific data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "c5528d51-9d73-4993-8bdf-e5bc7ad69c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data...\n",
      "2. Training Random Forest...\n",
      "3. Applying Veteran Guardrails...\n",
      "4. Saving Report...\n",
      "SUCCESS! Veteran Report saved as 'Final_Report_Veteran.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "THRESHOLD = 0.40 \n",
    "\n",
    "# STRICT SAFE LIST \n",
    "DB_SAFE_CATEGORIES = ['Dining', 'Groceries', 'Transport']\n",
    "\n",
    "# VETERAN LOYALTY \n",
    "# Only trust users who have been subscribed for > 1 Year.\n",
    "# Everyone else gets flagged if they have a zombie charge.\n",
    "LOYALTY_DAYS = 365 \n",
    "\n",
    "# --- FEATURE ENGINEERING ---\n",
    "def engineer_features(raw_df):\n",
    "    df = raw_df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(by=['user_id', 'merchant', 'timestamp'])\n",
    "    \n",
    "    # Time Deltas\n",
    "    df['prev_date'] = df.groupby(['user_id', 'merchant'])['timestamp'].shift(1)\n",
    "    df['days_diff'] = (df['timestamp'] - df['prev_date']).dt.days.fillna(999)\n",
    "    \n",
    "    # Loyalty\n",
    "    df['first_date'] = df.groupby(['user_id', 'merchant'])['timestamp'].transform('min')\n",
    "    df['relationship_days'] = (df['timestamp'] - df['first_date']).dt.days\n",
    "    \n",
    "    # Amount Deltas\n",
    "    df['prev_amount'] = df.groupby(['user_id', 'merchant'])['amount'].shift(1)\n",
    "    df['first_amount'] = df.groupby(['user_id', 'merchant'])['amount'].transform('first')\n",
    "    df['same_amount'] = (df['amount'] - df['prev_amount']).abs() < 0.01\n",
    "    \n",
    "    # Price Changes\n",
    "    df['price_change_pct'] = ((df['amount'] - df['prev_amount']) / df['prev_amount']).fillna(0)\n",
    "    df['total_change_pct'] = np.where(\n",
    "        df['first_amount'] > 0,\n",
    "        (df['amount'] - df['first_amount']) / df['first_amount'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Quick Charge\n",
    "    df['is_quick_charge'] = df['days_diff'] <= 5\n",
    "    \n",
    "    # Infer Frequency\n",
    "    def infer_freq(d):\n",
    "        if d == 999: return \"First_Txn\"\n",
    "        if 25 <= d <= 35: return \"Monthly\"\n",
    "        if 6 <= d <= 8: return \"Weekly\"\n",
    "        return \"Irregular\"\n",
    "    df['frequency'] = df['days_diff'].apply(infer_freq)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "print(\"1. Loading Data...\")\n",
    "df_train = engineer_features(pd.read_csv(\"mock_transactions.csv\"))\n",
    "df_test = engineer_features(pd.read_csv(\"mock_transactions-2.csv\"))\n",
    "\n",
    "df_train['target'] = df_train['pattern_label'] != 'normal'\n",
    "\n",
    "# --- TRAIN RANDOM FOREST ---\n",
    "print(\"2. Training Random Forest...\")\n",
    "features = ['amount', 'price_change_pct', 'total_change_pct', 'days_diff', 'is_quick_charge', 'frequency', 'category']\n",
    "X_train = df_train[features]\n",
    "y_train = df_train['target']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['frequency', 'category']),\n",
    "        ('num', 'passthrough', ['amount', 'price_change_pct', 'total_change_pct', 'days_diff'])\n",
    "    ])\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=8, min_samples_leaf=5, random_state=42))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- APPLY VETERAN LOGIC ---\n",
    "print(\"3. Applying Veteran Guardrails...\")\n",
    "X_test = df_test[features]\n",
    "\n",
    "# Base AI Prediction\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "df_test['ai_risk_score'] = probs\n",
    "df_test['final_verdict'] = probs >= THRESHOLD\n",
    "df_test['reason'] = \"AI Probability Match\"\n",
    "\n",
    "# Apply Rules\n",
    "for idx, row in df_test.iterrows():\n",
    "    \n",
    "    # Rule 1: STRICT Double Billing\n",
    "    if row['is_quick_charge'] and row['same_amount']:\n",
    "        if row['category'] not in DB_SAFE_CATEGORIES:\n",
    "            df_test.at[idx, 'ai_risk_score'] = 0.99 \n",
    "            df_test.at[idx, 'final_verdict'] = True\n",
    "            df_test.at[idx, 'reason'] = \"Guardrail: Double Billing Detected\"\n",
    "            \n",
    "    # Rule 2: VETERAN Zombie Detection (Gap > 60 days)\n",
    "    elif row['days_diff'] > 60 and row['days_diff'] < 999 and row['same_amount']:\n",
    "        # MIDDLE GROUND: Only trust Veterans (> 365 days)\n",
    "        if row['relationship_days'] > LOYALTY_DAYS:\n",
    "            pass # Trusted Veteran - Safe Pause\n",
    "        else:\n",
    "            # User < 1 Year? FLAG IT.\n",
    "            df_test.at[idx, 'ai_risk_score'] = 0.95\n",
    "            df_test.at[idx, 'final_verdict'] = True\n",
    "            df_test.at[idx, 'reason'] = \"Guardrail: Zombie Charge Detected\"\n",
    "\n",
    "# --- GENERATE REPORT ---\n",
    "print(\"4. Saving Report...\")\n",
    "output_filename = \"Final_Report_Veteran.csv\" \n",
    "grouped = df_test.groupby(['user_id', 'merchant'])\n",
    "results = []\n",
    "\n",
    "for (user, merchant), group in grouped:\n",
    "    latest = group.iloc[-1]\n",
    "    \n",
    "    # History JSON\n",
    "    history = []\n",
    "    for _, r in group.iterrows():\n",
    "        history.append({\n",
    "            \"date\": r['timestamp'].strftime('%Y-%m-%d'),\n",
    "            \"amt\": r['amount'],\n",
    "            \"freq\": r['frequency']\n",
    "        })\n",
    "    \n",
    "    is_predatory = bool(latest['final_verdict'])\n",
    "    confidence = round(latest['ai_risk_score'] * 100, 1)\n",
    "    \n",
    "    if is_predatory:\n",
    "        verdict = \"FLAGGED\"\n",
    "        conf_str = f\"Risk Level: {confidence}%\"\n",
    "        pattern = \"AI + Hybrid Logic\"\n",
    "        ai_reason = f\"{latest['reason']} ({confidence}%).\"\n",
    "    else:\n",
    "        verdict = \"Safe\"\n",
    "        conf_str = f\"Safety Score: {round(100 - confidence, 1)}%\"\n",
    "        pattern = \"None\"\n",
    "        ai_reason = f\"Safe (Confidence {confidence}%).\"\n",
    "\n",
    "    results.append({\n",
    "        \"Verdict\": verdict,\n",
    "        \"User\": user,\n",
    "        \"Merchant\": merchant,\n",
    "        \"Category\": latest['category'],\n",
    "        \"Pattern_Detected\": pattern,\n",
    "        \"AI_Confidence\": conf_str,\n",
    "        \"AI_Reason\": ai_reason,\n",
    "        \"Start_Date\": history[0]['date'],\n",
    "        \"Last_Txn\": history[-1]['date']\n",
    "    })\n",
    "\n",
    "final_df = pd.DataFrame(results).sort_values(by='Verdict', ascending=True)\n",
    "final_df.to_csv(output_filename, index=False)\n",
    "print(f\"SUCCESS! Veteran Report saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "38b913f6-a3eb-4808-bd38-d2d45fa7e839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INITIALIZING GOVERNANCE CLIENT ---\n",
      "Connected to Project ID: a0cfb053-a3b6-4b55-9d97-8348e29c8a98\n",
      "Loaded Governance Sheet: 100 records.\n",
      "\n",
      "--- FAIRNESS AUDIT REPORT ---\n",
      "High Income Flag Rate: 31.5%\n",
      "Low Income Flag Rate:  15.2%\n",
      "Disparate Impact Ratio: 0.48\n",
      "Result: WARNING (Potential bias detected)\n",
      "\n",
      "Registering Model to IBM Watsonx...\n",
      "Registration Successful. Model ID: e27d7ef1-aa8b-4288-b64c-121b89ab3c57\n",
      "Creating data asset...\n",
      "SUCCESS\n",
      "Validation Log Uploaded Successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "print(\"--- INITIALIZING GOVERNANCE CLIENT ---\")\n",
    "\n",
    "# SETUP CLIENT\n",
    "wml_credentials = {\n",
    "    \"url\": \"https://ca-tor.ml.cloud.ibm.com\", \n",
    "    \"apikey\": \"OP1mOPIL34elWjPUuCyDwsPl-3lUMt8MOYm3NKjgAyeO\" \n",
    "}\n",
    "\n",
    "try:\n",
    "    client = APIClient(wml_credentials)\n",
    "    \n",
    "    # Set the default project\n",
    "    if os.environ.get('PROJECT_ID'):\n",
    "        client.set.default_project(os.environ.get('PROJECT_ID'))\n",
    "        print(f\"Connected to Project ID: {os.environ.get('PROJECT_ID')}\")\n",
    "    else:\n",
    "        print(\"Error: PROJECT_ID not found in environment.\")\n",
    "\n",
    "    # LOAD GOVERNANCE DATA\n",
    "    gov_filename = \"Kinghacks governance sheet.csv\"\n",
    "    if not os.path.exists(gov_filename):\n",
    "        print(f\"Warning: {gov_filename} not found in local directory.\")\n",
    "        \n",
    "    df_gov = pd.read_csv(gov_filename)\n",
    "    print(f\"Loaded Governance Sheet: {len(df_gov)} records.\")\n",
    "\n",
    "    # FEATURE ENGINEERING\n",
    "    df_gov['amount'] = df_gov['Amount']\n",
    "    df_gov['price_change_pct'] = df_gov['Price_Change_Pct']\n",
    "    df_gov['total_change_pct'] = df_gov['Price_Change_Pct'] \n",
    "    df_gov['category'] = df_gov['Category']\n",
    "    df_gov['frequency'] = df_gov['Frequency']\n",
    "    \n",
    "    freq_map = {'Monthly': 30, 'Weekly': 7, 'Annually': 365, 'Irregular': 15}\n",
    "    df_gov['days_diff'] = df_gov['Frequency'].map(freq_map).fillna(30)\n",
    "    df_gov['is_quick_charge'] = False \n",
    "    \n",
    "    features = ['amount', 'price_change_pct', 'total_change_pct', 'days_diff', 'is_quick_charge', 'frequency', 'category']\n",
    "    X_gov = df_gov[features]\n",
    "\n",
    "    # GENERATE RISK SCORES\n",
    "    probs = model.predict_proba(X_gov)[:, 1]\n",
    "    df_gov['AI_Risk_Score'] = probs\n",
    "    df_gov['AI_Flagged'] = probs >= 0.40 \n",
    "\n",
    "    # PERFORM FAIRNESS AUDIT\n",
    "    stats = df_gov.groupby('Income_Bracket')['AI_Flagged'].mean()\n",
    "    low_rate = stats.get('Low', 0)\n",
    "    high_rate = stats.get('High', 0)\n",
    "    \n",
    "    disparate_impact = low_rate / high_rate if high_rate > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- FAIRNESS AUDIT REPORT ---\")\n",
    "    print(f\"High Income Flag Rate: {high_rate:.1%}\")\n",
    "    print(f\"Low Income Flag Rate:  {low_rate:.1%}\")\n",
    "    print(f\"Disparate Impact Ratio: {disparate_impact:.2f}\")\n",
    "\n",
    "    if 0.8 <= disparate_impact <= 1.25:\n",
    "        print(\"Result: PASS (Fairness within regulatory limits)\")\n",
    "    else:\n",
    "        print(\"Result: WARNING (Potential bias detected)\")\n",
    "\n",
    "    # REGISTER MODEL\n",
    "    print(\"\\nRegistering Model to IBM Watsonx...\")\n",
    "    \n",
    "    model_props = {\n",
    "        client.repository.ModelMetaNames.NAME: \"Veteran_Fraud_Detector_Final\",\n",
    "        \n",
    "        client.repository.ModelMetaNames.TYPE: \"scikit-learn_1.3\",\n",
    "        \n",
    "        client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: client.software_specifications.get_id_by_name(\"runtime-24.1-py3.11\"),\n",
    "        client.repository.ModelMetaNames.DESCRIPTION: f\"Veteran Logic. Fairness Score: {disparate_impact:.2f}\",\n",
    "        client.repository.ModelMetaNames.TAGS: [\"governance\", \"production\", \"kinghacks\"]\n",
    "    }\n",
    "\n",
    "    published_model = client.repository.store_model(\n",
    "        model=model, \n",
    "        meta_props=model_props, \n",
    "        training_data=X_train, \n",
    "        training_target=y_train\n",
    "    )\n",
    "    \n",
    "    model_id = client.repository.get_model_id(published_model)\n",
    "    print(f\"Registration Successful. Model ID: {model_id}\")\n",
    "\n",
    "    # UPLOAD VALIDATION ARTIFACTS\n",
    "    output_filename = \"Governance_Validation_Results.csv\"\n",
    "    df_gov.to_csv(output_filename, index=False)\n",
    "    client.data_assets.create(name=\"Governance_Validation_Log\", file_path=output_filename)\n",
    "    print(\"Validation Log Uploaded Successfully.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Error: Library 'ibm_watsonx_ai' not found. Please install it using: !pip install ibm-watsonx-ai\")\n",
    "except NameError:\n",
    "    print(\"Error: 'model', 'X_train', or 'y_train' not defined. Ensure Cell 5 has been executed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "62777774-12e8-4749-be79-cacdbcb6a274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on New Dataset: 96.26%\n"
     ]
    }
   ],
   "source": [
    "# Quick Accuracy Check\n",
    "df_ai = pd.read_csv(\"smart_analysis_new_dataset.csv\")\n",
    "df_truth = pd.read_csv(\"mock_transactions-2.csv\")\n",
    "\n",
    "# Extract AI Verdict\n",
    "df_ai['Predatory_AI'] = df_ai['Model_Analysis'].apply(lambda x: json.loads(x)['is_predatory'])\n",
    "\n",
    "# Extract Truth Verdict\n",
    "ground_truth = df_truth.groupby(['user_id', 'merchant'])['pattern_label'].agg(lambda x: x.mode()[0]).reset_index()\n",
    "ground_truth['Predatory_Truth'] = ground_truth['pattern_label'] != 'normal'\n",
    "\n",
    "# Compare\n",
    "comparison = pd.merge(df_ai, ground_truth, left_on=['User', 'Merchant'], right_on=['user_id', 'merchant'])\n",
    "correct = comparison[comparison['Predatory_AI'] == comparison['Predatory_Truth']]\n",
    "\n",
    "print(f\"Accuracy on New Dataset: {len(correct) / len(comparison) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8364e6b4-abdc-4667-8422-38674c67f74e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading readable report...\n",
      "Creating data asset...\n",
      "SUCCESS\n",
      "SUCCESS! File uploaded to Project Assets.\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading readable report...\")\n",
    "try:\n",
    "    client.data_assets.create(\n",
    "        name=\"Final_Report_Veteran.csv\",\n",
    "        file_path=\"Final_Report_Veteran.csv\"\n",
    "    )\n",
    "    print(\"SUCCESS! File uploaded to Project Assets.\")\n",
    "except Exception as e:\n",
    "    print(f\"Upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "06b3172e-5961-427a-b160-eb41bfeecdc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PROMOTING & DEPLOYING ---\n",
      "Unsetting the project_id ...\n",
      "Target Space Set: d02fd2a8-db85-44b0-a4e5-085a360521b8\n",
      "Promoting model to Production Space...\n",
      "Model Promoted! New Asset ID: 972ba549-dd6b-4742-a8bd-b25993b60d4d\n",
      "Checking for existing deployments...\n",
      "Creating new deployment...\n",
      "\n",
      "\n",
      "######################################################################################\n",
      "\n",
      "Synchronous deployment creation for id: '972ba549-dd6b-4742-a8bd-b25993b60d4d' started\n",
      "\n",
      "######################################################################################\n",
      "\n",
      "\n",
      "initializing\n",
      "Note: online_url and serving_urls are deprecated and will be removed in a future release. Use inference instead.\n",
      "..\n",
      "ready\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Successfully finished deployment creation, deployment_id='92ffc9fd-afa7-47b3-af34-0f8f22b440a0'\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Note: online_url and serving_urls are deprecated and will be removed in a future release. Use inference instead.\n",
      "\n",
      "SUCCESS! Model Deployed.\n",
      "Deployment ID: 92ffc9fd-afa7-47b3-af34-0f8f22b440a0\n",
      "API Endpoint:  https://ca-tor.ml.cloud.ibm.com/ml/v4/deployments/92ffc9fd-afa7-47b3-af34-0f8f22b440a0/predictions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "SPACE_ID = \"d02fd2a8-db85-44b0-a4e5-085a360521b8\" \n",
    "\n",
    "\n",
    "print(\"--- PROMOTING & DEPLOYING ---\")\n",
    "\n",
    "# SET TARGET SPACE\n",
    "client.set.default_space(SPACE_ID)\n",
    "print(f\"Target Space Set: {SPACE_ID}\")\n",
    "\n",
    "# PROMOTE MODEL (Project -> Space)\n",
    "try:\n",
    "    print(\"Promoting model to Production Space...\")\n",
    "    \n",
    "    # Get the source project ID\n",
    "    proj_id = os.environ.get('PROJECT_ID')\n",
    "    \n",
    "    promoted_asset_id = client.spaces.promote(\n",
    "        asset_id=model_id, \n",
    "        source_project_id=proj_id, \n",
    "        target_space_id=SPACE_ID\n",
    "    )\n",
    "    print(f\"Model Promoted! New Asset ID: {promoted_asset_id}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Note: Promotion skipped or failed ({str(e)}). Checking if model exists in space...\")\n",
    "    \n",
    "    # List models in the space to find the ID\n",
    "    client.set.default_space(SPACE_ID)\n",
    "    models_details = client.repository.get_details()\n",
    "    \n",
    "    promoted_asset_id = None\n",
    "    if 'models' in models_details and 'resources' in models_details['models']:\n",
    "        for m in models_details['models']['resources']:\n",
    "            if m['metadata']['name'] == \"Veteran_Fraud_Detector_Final\":\n",
    "                promoted_asset_id = m['metadata']['id']\n",
    "                print(f\"Found existing model in space: {promoted_asset_id}\")\n",
    "                break\n",
    "    \n",
    "    if not promoted_asset_id:\n",
    "        raise Exception(\"CRITICAL: Model not found in Space and Promotion failed.\")\n",
    "\n",
    "# DEPLOY\n",
    "deploy_meta = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Veteran_Fraud_API_v1\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "# Check for existing deployments to avoid 'Conflict' errors\n",
    "print(\"Checking for existing deployments...\")\n",
    "existing_deployments = client.deployments.get_details()\n",
    "deployment_id = None\n",
    "\n",
    "if 'resources' in existing_deployments:\n",
    "    for d in existing_deployments['resources']:\n",
    "        if d['entity']['name'] == \"Veteran_Fraud_API_v1\":\n",
    "            deployment_id = d['metadata']['id']\n",
    "            print(f\"Found existing deployment: {deployment_id}\")\n",
    "            break\n",
    "\n",
    "if deployment_id is None:\n",
    "    print(\"Creating new deployment...\")\n",
    "    deployment_details = client.deployments.create(\n",
    "        artifact_uid=promoted_asset_id, \n",
    "        meta_props=deploy_meta\n",
    "    )\n",
    "    deployment_id = client.deployments.get_uid(deployment_details)\n",
    "\n",
    "# GET URL\n",
    "scoring_url = client.deployments.get_scoring_href(client.deployments.get_details(deployment_id))\n",
    "\n",
    "print(f\"\\nSUCCESS! Model Deployed.\")\n",
    "print(f\"Deployment ID: {deployment_id}\")\n",
    "print(f\"API Endpoint:  {scoring_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c11ff35e-546c-405d-8f4b-f8329d73e307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 'Nuclear' Test Scenarios to Watsonx...\n",
      "\n",
      "--- DIAGNOSTIC RESULTS (576.7ms) ---\n",
      "\n",
      "Scenario 1 (Price Hike 50.0):\n",
      "Fraud Score: 0.80\n",
      "Verdict: ðŸ›‘ FLAGGED\n",
      "\n",
      "Scenario 2 (Rapid Fire Attack):\n",
      "Fraud Score: 0.73\n",
      "Verdict: ðŸ›‘ FLAGGED\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 2 scenarios\n",
    "payload_data = {\n",
    "    \"input_data\": [{\n",
    "        \"fields\": ['amount', 'price_change_pct', 'total_change_pct', 'days_diff', 'is_quick_charge', 'frequency', 'category'],\n",
    "        \"values\": [\n",
    "            [24.99, 50.0, 50.0, 90, False, \"Irregular\", \"Entertainment\"],\n",
    "            \n",
    "            [99.99, 100.0, 100.0, 0, True, \"Irregular\", \"Software\"]\n",
    "        ]\n",
    "    }]\n",
    "}\n",
    "\n",
    "print(\"Sending 'Nuclear' Test Scenarios to Watsonx...\")\n",
    "start = time.time()\n",
    "predictions = client.deployments.score(deployment_id, payload_data)\n",
    "latency = round((time.time() - start) * 1000, 1)\n",
    "\n",
    "# READ RESULTS\n",
    "# The structure is: [Row][Column 1 (Probability List)][Index 1 (Fraud Score)]\n",
    "rows = predictions['predictions'][0]['values']\n",
    "\n",
    "print(f\"\\n--- DIAGNOSTIC RESULTS ({latency}ms) ---\")\n",
    "\n",
    "# --- SCENARIO 1 ---\n",
    "# rows[0] is the first transaction\n",
    "# rows[0][1] is the probability list: [0.15, 0.85]\n",
    "# rows[0][1][1] is the FRAUD probability: 0.85\n",
    "fraud_prob_1 = rows[0][1][1] \n",
    "\n",
    "print(f\"\\nScenario 1 (Price Hike 50.0):\")\n",
    "print(f\"Fraud Score: {fraud_prob_1:.2f}\")\n",
    "print(f\"Verdict: {'ðŸ›‘ FLAGGED' if fraud_prob_1 >= 0.40 else 'âœ… SAFE'}\")\n",
    "\n",
    "# --- SCENARIO 2 ---\n",
    "fraud_prob_2 = rows[1][1][1]\n",
    "\n",
    "print(f\"\\nScenario 2 (Rapid Fire Attack):\")\n",
    "print(f\"Fraud Score: {fraud_prob_2:.2f}\")\n",
    "print(f\"Verdict: {'ðŸ›‘ FLAGGED' if fraud_prob_2 >= 0.40 else 'âœ… SAFE'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "759ba99b-43a1-45e4-939f-ccf7082b45fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8218230-7801-4d30-a821-7fda956c2ce3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "433feb3a-a27a-43c8-b5a9-734d6f5d1844"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd025f7e-c321-471c-97ca-d174c5e24460"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "905aea76-7dab-4bb1-804c-b365180af6c6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86909d46-1fe8-440d-9f5d-aaf4101636de"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
